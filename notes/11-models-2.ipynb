{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1707312228958,
     "user": {
      "displayName": "Lutz Hamel",
      "userId": "10287662568849688016"
     },
     "user_tz": 300
    },
    "id": "n8WKWoVMYqMy",
    "outputId": "184518ea-d365-482b-b65f-e1a2022654b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "###### Config #####\n",
    "import sys, os, platform\n",
    "if os.path.isdir(\"ds-assets\"):\n",
    "  !cd ds-assets && git pull\n",
    "else:\n",
    "  !git clone https://github.com/lutzhamel/ds-assets.git\n",
    "colab = True if 'google.colab' in os.sys.modules else False\n",
    "system = platform.system() # \"Windows\", \"Linux\", \"Darwin\"\n",
    "home = \"ds-assets/assets/\"\n",
    "sys.path.append(home)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook level imports\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn import metrics \n",
    "from sklearn import model_selection \n",
    "import seaborn as sns; sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "j-Vbdm-RKsFE"
   },
   "outputs": [],
   "source": [
    "# format output from library calls\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float_kind':\"{:3.2f}\".format})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjsCjE04YqM6"
   },
   "source": [
    "# Evaluating Models\n",
    "\n",
    "* You migh be wondering at this point why your models when left unrestricted\n",
    "(max_depth=None) always get a perfect score or something close to it.\n",
    "\n",
    "* Consider the iris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 100.00%\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(home+\"iris.csv\")\n",
    "X  = df.drop(columns=['id','Species'])\n",
    "y = df[['Species']]\n",
    "\n",
    "acc = tree\\\n",
    "   .DecisionTreeClassifier(max_depth=None)\\\n",
    "   .fit(X, y)\\\n",
    "   .score(X, y)\n",
    "\n",
    "print(f\"accuracy = {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Turns out that this is a well known phenomenon in machine learning\n",
    "\n",
    "* This can be characterized by **learning curves**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpGoX5wvYqM6"
   },
   "source": [
    "## Learning Curves\n",
    "\n",
    "* Learning curves illustrate the general trends of learners. \n",
    "* The **blue line** illustrates what will happen when we **train and test** the model\n",
    "   with the **same data** (like we did above) - we call that the **training score**.\n",
    "* The **red line** illustrates what will happen if we **test** our model with **separate\n",
    "   data**, different from the training data - we call that the **testing score**.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/lutzhamel/ds-assets/main/assets/train-test-curves.png\"  height=\"300\" width=\"450\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpfBnlIgYqM8"
   },
   "source": [
    "\n",
    "* It can be shown that any model can learn its training data perfectly - “memorize it”. \n",
    "* Any model can achieve a perfect score on the training data as long as it is allowed to be complex enough. \n",
    "* That is what the blue curve shows above. \n",
    "\n",
    "BUT\n",
    "\n",
    "* memorizing is not the same as learning inherent patterns\n",
    "* Memorization is extremely bad at predicting labels for data that it hasn't seen yet.  \n",
    "* Notice in the graph, models that have perfect training score perform poorly on the test data\n",
    "* That is what the red curve above shows\n",
    "* We say,\n",
    "\n",
    "<center>\n",
    "\n",
    "**Memorization does not generalize well!**\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Simply put:\n",
    "\n",
    "1. Undertrained models (low complexity models) make a lot of errors on test data because they have not learned any of the patterns yet.\n",
    "\n",
    "2. Overtrained models (high complexity models) make a lot of errors on test data because memorization is extremely bad at predicting labels on data they haven't been trained on.\n",
    "\n",
    "3. The best models make a trade-off between errors and recognizing important patterns. **Notice that for the best models the training score is not 100%!**\n",
    "\n",
    "\n",
    "\n",
    "**Observation**: In order to find the **best model** we have to **search the model space** to find just the right complexity level. We \n",
    "control the model space via the appropriate parameter settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYglFKc2YqM9"
   },
   "source": [
    "## Searching the Model Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will demonstrate the model search using decision trees with both the iris and wisconsin breast cancer datasets.\n",
    "\n",
    "For each dataset we'll do the following:\n",
    "1. We'll split the data into  training and testing partitions\n",
    "2. Create trees from low complexity to high complexity\n",
    "3. Train each of these model and test it (testing score)\n",
    "4. We pick the model that has the **highest testing score**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yx-cHZu7YqM-"
   },
   "source": [
    "### The Iris Dataset\n",
    "\n",
    "* We start with the iris dataset.  \n",
    "* We would expect a lower testing accuracy from both the low-complexity and high-complexity models compared to a medium-complexity model\n",
    "* The medium-complexity model is most likely our best model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "Gkq8K9pYYqM-"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(home+\"iris.csv\")\n",
    "XI  = df.drop(columns=['id','Species'])\n",
    "yI = df[['Species']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and testing data partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split the data - 70% training 30% testing\n",
    "(XI_train, XI_test, yI_train, yI_test) = \\\n",
    "    model_selection.train_test_split(XI, \n",
    "                                     yI, \n",
    "                                     train_size=0.7, \n",
    "                                     test_size=0.3, \n",
    "                                     random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to figure out how complex the max complexity tree is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_complexity = tree\\\n",
    "   .DecisionTreeClassifier(max_depth=None)\\\n",
    "   .fit(XI_train, yI_train)\\\n",
    "   .tree_\\\n",
    "   .max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a search over the complexity of trees,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth=1          train score=0.67           test score=0.67\n",
      "max_depth=2          train score=0.96           test score=0.96\n",
      "max_depth=3          train score=0.98           test score=0.98\n",
      "max_depth=4          train score=1.00           test score=0.96\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,max_complexity+1):\n",
    "    # build a model with appropriate complexity\n",
    "    model = tree.DecisionTreeClassifier(max_depth=i).fit(XI_train, yI_train)\n",
    "    # training score\n",
    "    acc_train = model.score(XI_train, yI_train)\n",
    "    # testing score\n",
    "    acc_test = model.score(XI_test, yI_test)\n",
    "    # print results\n",
    "    print(f\"max_depth={i}\\\n",
    "          train score={acc_train:.2f} \\\n",
    "          test score={acc_test:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: \n",
    "* From the list above, our best tree is a tree with **max_depth=3**. \n",
    "* It has the **highest test score**\n",
    "* Also notice that the training and testing scores behave just as predicted by the \n",
    "   learning curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Is2IM9NYqNC"
   },
   "source": [
    "### Wisconsin Breast Cancer Dataset\n",
    "\n",
    "Let's try this again with a slightly larger datasest. This data set is available at <a href=\"https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\">UCI</a>.\n",
    "The data set describes benign and malignent tumors based on image measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1707312231488,
     "user": {
      "displayName": "Lutz Hamel",
      "userId": "10287662568849688016"
     },
     "user_tz": 300
    },
    "id": "Y9Z8F6qrYqNC",
    "outputId": "736bed53-8989-4718-fb98-879ace7f1487"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(home+\"wdbc.csv\")\n",
    "XW  = df.drop(columns=['ID','Diagnosis'])\n",
    "yW = df[['Diagnosis']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data - 70% training 30% testing\n",
    "(XW_train, XW_test, yW_train, yW_test) = \\\n",
    "    model_selection.train_test_split(XW, \n",
    "                                     yW, \n",
    "                                     train_size=0.7, \n",
    "                                     test_size=0.3, \n",
    "                                     random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_complexity = tree\\\n",
    "   .DecisionTreeClassifier(max_depth=None)\\\n",
    "   .fit(XW_train, yW_train)\\\n",
    "   .tree_\\\n",
    "   .max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching for the best tree for the Wisconsin data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth=1          train score=0.93           test score=0.90\n",
      "max_depth=2          train score=0.96           test score=0.91\n",
      "max_depth=3          train score=0.97           test score=0.93\n",
      "max_depth=4          train score=0.99           test score=0.95\n",
      "max_depth=5          train score=1.00           test score=0.94\n",
      "max_depth=6          train score=1.00           test score=0.92\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,max_complexity+1):\n",
    "    # build a model with appropriate complexity\n",
    "    model = tree\\\n",
    "        .DecisionTreeClassifier(max_depth=i,random_state=3)\\\n",
    "        .fit(XW_train, yW_train)\n",
    "    # training score\n",
    "    acc_train = model.score(XW_train, yW_train)\n",
    "    # testing score\n",
    "    acc_test = model.score(XW_test, yW_test)\n",
    "    # print results\n",
    "    print(f\"max_depth={i}\\\n",
    "          train score={acc_train:.2f} \\\n",
    "          test score={acc_test:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**:\n",
    "* Our best model is a tree with **max_depth=4**\n",
    "* It has the **highest test score**\n",
    "* Again, the training and testing scores behave just as predicted by the \n",
    "   learning curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIJS_MTJYqNJ"
   },
   "source": [
    "## Automating the Search: The Grid Search\n",
    "\n",
    "* As we saw above, the only way to find the best model for a particular dataset is to search for it by trying different parameters that control the complexity of the models.  \n",
    "* **Model complexity is often governed by more than one parameter**, therefore\n",
    "* The model search is usually referred to as the **grid search**.\n",
    "* The **sklearn GridSearchCV** function automates the model search\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gd2jmal2YqNK"
   },
   "source": [
    "### Grid Search with sklearn\n",
    "\n",
    "* Sklearn has a built-in grid search that searches the models space and \n",
    "   returns the **best model**\n",
    "* In our case the decision tree classifiers are governed by max_depth. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris Data\n",
    "Grid search for best model for iris data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up grid search\n",
    "depth_ceiling = tree\\\n",
    "    .DecisionTreeClassifier(max_depth=None)\\\n",
    "    .fit(XI_train, yI_train)\\\n",
    "    .tree_\\\n",
    "    .max_depth\n",
    "model = tree.DecisionTreeClassifier(random_state=3)\n",
    "param = {\n",
    "   'max_depth': list(range(1,depth_ceiling+1))\n",
    "   }              \n",
    "best_model = model_selection\\\n",
    "    .GridSearchCV(model, param)\\\n",
    "    .fit(XI_train,yI_train)\\\n",
    "    .best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth=3 \t Testing Score=0.98\n"
     ]
    }
   ],
   "source": [
    "# compute the accuracy of optimal classifier\n",
    "acc = best_model.score(XI_test,yI_test)\n",
    "depth = best_model.tree_.max_depth\n",
    "print(f\"Depth={depth} \\t Testing Score={acc:3.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wisconsin Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search for best model for wisconsin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up grid search\n",
    "depth_ceiling = tree\\\n",
    "    .DecisionTreeClassifier(max_depth=None)\\\n",
    "    .fit(XW_train, yW_train)\\\n",
    "    .tree_\\\n",
    "    .max_depth\n",
    "model = tree.DecisionTreeClassifier(random_state=3)\n",
    "param = {\n",
    "    'max_depth': list(range(1,depth_ceiling+1)),               \n",
    "    }\n",
    "best_model = model_selection\\\n",
    "    .GridSearchCV(model, param)\\\n",
    "    .fit(XW_train,yW_train)\\\n",
    "    .best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth=5 \t Testing Score=0.94\n"
     ]
    }
   ],
   "source": [
    "# compute the accuracy of optimal classifier\n",
    "acc = best_model.score(XW_test,yW_test)\n",
    "depth = best_model.tree_.max_depth\n",
    "print(f\"Depth={depth} \\t Testing Score={acc:3.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Refit Score\n",
    "\n",
    "* Sklearn's grid search function performs its **own internal train-test split** in order to search for the best model\n",
    "* That means there is no need for us to perform a manual train-test split\n",
    "* We can simply use the whole data set in the search\n",
    "* This will make our search much simpler\n",
    "\n",
    "Question:\n",
    "\n",
    "* How do we evaluate the model if we used the whole data set for model searching?\n",
    "\n",
    "Answer:\n",
    "\n",
    "* We use the **whole data set for evaluation**\n",
    "* This is known as **refit**\n",
    "* The score we obtain is called the **refit score**\n",
    "\n",
    "Observation:\n",
    "\n",
    "* **There is no danger of overfitting because the grid search performed internal\n",
    "   train-test splits**\n",
    "* We will later develop the tools to demonstrate that the refit score is\n",
    "   statistically the same as a formal train-test evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model to the entire dataset\n",
    "depth_ceiling = tree.DecisionTreeClassifier(max_depth=None)\\\n",
    "   .fit(XI, yI)\\\n",
    "   .tree_\\\n",
    "   .max_depth\n",
    "model = tree.DecisionTreeClassifier()\n",
    "param_grid = {\n",
    "    'max_depth': list(range(1,depth_ceiling+1))              \n",
    "    }\n",
    "best_model = model_selection\\\n",
    "   .GridSearchCV(model, param_grid)\\\n",
    "   .fit(XI,yI)\\\n",
    "   .best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth=4 \t Accuracy=0.99\n"
     ]
    }
   ],
   "source": [
    "# evaluate the best model\n",
    "acc = best_model.score(XI,yI)\n",
    "depth = best_model.tree_.max_depth\n",
    "print(f\"Depth={depth} \\t Accuracy={acc:3.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dAakZWcYqNL"
   },
   "source": [
    "# Model Accuracy Reexamined\n",
    "\n",
    "The accuracy score for classifiers we have been looking at so far is a good first look at the \n",
    "performance of a classifier.  However, for sensitive classification tasks like biomedical applications\n",
    "we would like to understand the errors a classifier makes a little bit better.\n",
    "\n",
    "Consider a classification problem with two classes, then we can observe the following outcomes of a prediction of a classification model:\n",
    "\n",
    ">**true positive (TP)** -- predicted positive coincides with actual positive\n",
    ">\n",
    ">**true negative (TN)** -- predicted negative coincides with actual negative\n",
    ">\n",
    ">**false positive (FP)** -- predicted positive but actual negative (Type I error)\n",
    ">\n",
    ">**false negative (FN)** -- predicted negative but actual positive (Type II error)\n",
    ">\n",
    "\n",
    "**Observation**: Two types of errors possible!\n",
    "\n",
    "The distinction between these two types of errors is extremely important.  Consider a biomedical diagnostic\n",
    "decision model predicting the presence of a disease.  A false positive tends to not be problematic in this context because it will simply lead to more tests until it is discovered that the model make a false positive prediction.  The false negative prediction is much more troublesome; the patient is told to be disease free when in fact they are not.  Therefore, practitioners who build models for these kind of sensitive applications try to minimize false negative predictions of the models as much as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3LEtcjRYqNL"
   },
   "source": [
    "### The Confusion Matrix\n",
    "\n",
    "An easy way to visualize the four outcomes of a binary decision model is the **confusion matrix**.\n",
    "* We can arrange the predictions in a matrix form\n",
    "* Errors will show up as values outside the major diagonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ws6mbE8TYqNL"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/lutzhamel/ds-assets/main/assets/confusion2.png\" height=\"200\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0g6hVA9YqNM"
   },
   "source": [
    "## The Wisconsin Breast Cancer Data Set\n",
    "\n",
    "Let's look at the performance of a tree model for the Wisconsin Breast Cancer Dataset using a confusion matrix.  We will evaluate the optimal tree model for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3372,
     "status": "ok",
     "timestamp": 1707312238084,
     "user": {
      "displayName": "Lutz Hamel",
      "userId": "10287662568849688016"
     },
     "user_tz": 300
    },
    "id": "f3a_kG21YqNM",
    "outputId": "31763900-0c34-43a5-d9af-24b9676d97cc"
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "df = pd.read_csv(home+\"wdbc.csv\")\n",
    "X  = df.drop(columns=['ID','Diagnosis'])\n",
    "y = df[['Diagnosis']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_ceiling = tree.DecisionTreeClassifier().fit(X,y).tree_.max_depth\n",
    "model = tree.DecisionTreeClassifier(random_state=1)\n",
    "param = {\n",
    "    'max_depth': list(range(1,depth_ceiling+1)),\n",
    "    'criterion': ['gini','entropy']\n",
    "    }\n",
    "best_model = model_selection\\\n",
    "    .GridSearchCV(model,param)\\\n",
    "    .fit(X,y)\\\n",
    "    .best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=0.98\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy={best_model.score(X,y):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Here we take advantage of the fact that GridSearchCV can search over **multiple model parameters**.  Here, this makes a difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>210</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>7</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     M    B\n",
       "M  210    2\n",
       "B    7  350"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# M label top-left corner\n",
    "# we want the malignent tumor prediction to be our \"positive\"\n",
    "labels = [\n",
    "   'M', # Malignant - positive\n",
    "   'B'  # Benign - negative\n",
    "   ]\n",
    "\n",
    "# create predicted values for target\n",
    "predict_y = best_model.predict(X)\n",
    "\n",
    "# build the confusion matrix\n",
    "cm = metrics.confusion_matrix(y,             # observed target values (rows)\n",
    "                              predict_y,     # predicted target values (columns)\n",
    "                              labels=labels) # labels for arranging the cm\n",
    "\n",
    "# cm is just an array of values, turn it into something readable\n",
    "cm_df = pd.DataFrame(cm, \n",
    "                     index=labels, \n",
    "                     columns=labels)\n",
    "\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzN7cUPBgYap"
   },
   "source": [
    "We see that most of the instances lie on the major diagonal, that means the model predicted those instances correctly.  On the top line we also see that the model had **2 false negatives** (predicted malignant as benign) and on the bottom line it had **7 false positives** (predicted benign as malignant).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's interpret this matrix in more detail.  Summing accross the rows gives us the observed target values.\n",
    "\n",
    "* M: $210+2=212$\n",
    "* B: $7+350=357$\n",
    "\n",
    "Notice that this coincides precisely with the label counts in our data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Diagnosis\n",
       "B            357\n",
       "M            212\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Diagnosis']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an easy way to check that you set up your confusion matrix correcty!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObcLj7XGvWGQ"
   },
   "source": [
    "## The Iris Data Set\n",
    "\n",
    "Here we are building a **three way confusion matrix** because we have three classification labels.  We apply our grid search to find the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2208,
     "status": "ok",
     "timestamp": 1707312240290,
     "user": {
      "displayName": "Lutz Hamel",
      "userId": "10287662568849688016"
     },
     "user_tz": 300
    },
    "id": "xsrwjAt-vuYh",
    "outputId": "71caefc8-7367-4a93-f06d-6f1df8cd0a2c"
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "df = pd.read_csv(home+\"iris.csv\")\n",
    "X  = df.drop(columns=['id','Species'])\n",
    "y = df[['Species']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "depth_ceiling = tree.DecisionTreeClassifier().fit(X,y).tree_.max_depth\n",
    "model = model = tree.DecisionTreeClassifier(random_state=1)\n",
    "param = {\n",
    "    'max_depth': list(range(1,depth_ceiling+1))\n",
    "    }\n",
    "best_model = model_selection\\\n",
    "    .GridSearchCV(model,param)\\\n",
    "    .fit(X,y)\\\n",
    "    .best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>setosa</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versicolor</th>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virginica</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            setosa  versicolor  virginica\n",
       "setosa          50           0          0\n",
       "versicolor       0          50          0\n",
       "virginica        0           1         49"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build and print the confusion matrix\n",
    "labels = ['setosa','versicolor','virginica'] # labels in alphabetic order\n",
    "\n",
    "predict_y = best_model.predict(X)\n",
    "cm = metrics.confusion_matrix(y,         # observed\n",
    "                              predict_y, # predicted\n",
    "                              labels=labels)\n",
    "cm_df = pd.DataFrame(cm, \n",
    "                     index=labels, \n",
    "                     columns=labels)\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocIWBmB0isbb"
   },
   "source": [
    "In a three-way confusion matrix we usually do not talk about false positives or negatives.  We just **look for misclassifications and try to characterize them**.  In our case the model makes one mistake and misclassifies a single instance of virginica as a versicolor flower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84F_LVAOYqNM"
   },
   "source": [
    "# Reading\n",
    "\n",
    "5.3 [Hyperparameters and Model Validation](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
