{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1709650292841,
     "user": {
      "displayName": "Lutz Hamel",
      "userId": "10287662568849688016"
     },
     "user_tz": 300
    },
    "id": "eYWaaaNJfNH1",
    "outputId": "d6210fcb-796b-42c3-96c7-ca6f800925f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "###### Config #####\n",
    "import sys, os, platform\n",
    "if os.path.isdir(\"ds-assets\"):\n",
    "  !cd ds-assets && git pull\n",
    "else:\n",
    "  !git clone https://github.com/lutzhamel/ds-assets.git\n",
    "colab = True if 'google.colab' in os.sys.modules else False\n",
    "system = platform.system() # \"Windows\", \"Linux\", \"Darwin\"\n",
    "home = \"ds-assets/assets/\"\n",
    "sys.path.append(home)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1709650292842,
     "user": {
      "displayName": "Lutz Hamel",
      "userId": "10287662568849688016"
     },
     "user_tz": 300
    },
    "id": "74XIcu6lLroz"
   },
   "outputs": [],
   "source": [
    "# notebook level imports\n",
    "import pandas as pd\n",
    "import dsutils                        # classification_confint\n",
    "from sklearn import neural_network    # MLPClassifier\n",
    "from sklearn import metrics           # accuracy_score\n",
    "from sklearn import model_selection   # GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLDTyHiyfNH9"
   },
   "source": [
    "# Constructing a basic MLP\n",
    "\n",
    "**NOTE**: For the most part we will stick with the sklearn terminology and call ANNs **Multi Layer Perceptrons**.\n",
    "\n",
    "* A fundamental problem with MLP design is the **sheer number of design possibilities**.  \n",
    "* The sklearn MLP classisfier 23 (!) tunable parameters.  \n",
    "* The good news is that most of these parameters have good default values that we don't have to touch.\n",
    "* There are only a few parameters that have an extraordinary effect on the quality of MLP models:\n",
    "   1. The number of **layers** and **nodes** in the network\n",
    "   1. The **transfer/activation** function\n",
    "* The final thing we have to worry about is: **How often do we apply the training data to the network\n",
    "   until it is fully trained** -- the max_iter parameter.\n",
    "   * Unfortunately, here the default value of 200 is completely insufficient\n",
    "   * I usually set this to some very large value like **10000** or in some instances **100000**.\n",
    "   * The good news is, the network will give you a warning if max_iter was set too low\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Where to Start: The 'Rule of Thumb' Network\n",
    "\n",
    "A good place to start a network design is with a network consisting of a **single hidden layer** with a **number of nodes** computed as,\n",
    "\n",
    "$$\n",
    "N = 2 \\times V\n",
    "$$\n",
    "\n",
    "where $N$ is the number of nodes in the hidden layer and $V$ is the number of independent variables in \n",
    "the training data, e.g. V = X.shape[1].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's try this using the Wisconsin dataset,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1709650292842,
     "user": {
      "displayName": "Lutz Hamel",
      "userId": "10287662568849688016"
     },
     "user_tz": 300
    },
    "id": "FUyuHeUiLvn8"
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "df = pd.read_csv(home+\"wdbc.csv\").drop(columns=['ID'])\n",
    "\n",
    "X  = df.drop(columns=['Diagnosis'])\n",
    "y = df['Diagnosis']\n",
    "V = X.shape[1]\n",
    "N = 2*V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 713,
     "status": "ok",
     "timestamp": 1709650293553,
     "user": {
      "displayName": "Lutz Hamel",
      "userId": "10287662568849688016"
     },
     "user_tz": 300
    },
    "id": "EsFdJqQr8Za1",
    "outputId": "4509a539-87c6-43ac-d287-65ad1c08bcee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden layer size\n",
    "N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# a multi-layer perceptron with one hidden layer\n",
    "model = neural_network\\\n",
    "   .MLPClassifier(\n",
    "      hidden_layer_sizes=(N,),  # one hidden layer with N neurons\n",
    "      activation='logistic',    # logistic activation function\n",
    "      max_iter=10000, \n",
    "      random_state=1\n",
    "      )\\\n",
    "   .fit(X, y)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The logistic activation function is a great \"general purpose\" activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy: 0.95 (0.93, 0.96)'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "dsutils.acc_score(model, X, y, as_string=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "191fBfjf32tc"
   },
   "source": [
    "**Observation**: The accuracy of this classifier is encouraging given that we constructed it **using just our rule of thumb**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUORcmWDfNIA"
   },
   "source": [
    "## MLP Grid Search\n",
    "\n",
    "Let's construct an optimal model and compare its performance to our 'rule of thumb' network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 130356,
     "status": "ok",
     "timestamp": 1709650423907,
     "user": {
      "displayName": "Lutz Hamel",
      "userId": "10287662568849688016"
     },
     "user_tz": 300
    },
    "id": "655pgMsffNIA",
    "outputId": "dff0c685-1acf-4778-fdb3-19e37c65ea7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: best parameters: {'activation': 'logistic', 'hidden_layer_sizes': (120, 30)}\n"
     ]
    }
   ],
   "source": [
    "# neural network object\n",
    "model = neural_network.MLPClassifier(max_iter=10000, random_state=1)\n",
    "\n",
    "# grid search:\n",
    "#   * limit to 1 and 2 hidden layers\n",
    "#   * vary number of neurons in each layer (multiples of 2 of N)\n",
    "#   * vary activation functions\n",
    "param_grid = {\n",
    "    # search over different architectures\n",
    "    'hidden_layer_sizes':\n",
    "      [\n",
    "      # single layer MLP: vary size by N with multipliers of 2\n",
    "      (N//2,), (N,), (N*2,),\n",
    "      \n",
    "      # 2 layers: first fixed at N/2, second varying\n",
    "      (N//2,N//2), (N//2, N), (N//2, N*2),\n",
    "      \n",
    "      # 2 layers: first fixed at N, second varying\n",
    "      (N, N//2), (N,N), (N, N*2),\n",
    "      \n",
    "      # 2 layers: first N*2, second varying\n",
    "      (N*2, N//2), (N*2, N), (N*2, N*2)\n",
    "      ],\n",
    "    \n",
    "    # search different activation functions\n",
    "    'activation' : ['logistic',  'tanh', 'relu']  \n",
    "}\n",
    "\n",
    "# perform grid search\n",
    "grid = model_selection.GridSearchCV(model, param_grid).fit(X, y)\n",
    "best_params = grid.best_params_\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "print(f\"Grid Search: best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy: 0.96 (0.95, 0.98)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the best model\n",
    "dsutils.acc_score(best_model, X, y, as_string=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8RWVg-GTmGf"
   },
   "source": [
    "**Observations**: The difference in performance between the optimal MLP and our 'rule of thumb' MLP is **not** statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We see this a lot with complex models where 'rule of thumb' models come very close to the optimal performance.  \n",
    "\n",
    ">Therefore, practioners often forgo the search for optimal models and use 'rule of thumb' models which they then tweak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions Reviewed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic:<br>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1920px-Logistic-curve.svg.png\"\n",
    "width=\"300\" height=\"200\">\n",
    "\n",
    "Hyperbolic-Tangent:<br>\n",
    "<img src=\"https://media.datacamp.com/cms/google/ad_4nxccvibwobiyw2uym_egwqg8pf6zmnygb_mqy3khokh_3biaex50ultijygp7wg_13qdnddbbd2yevutty96pcimrj3hdihnpv-vsjopo4wyvfpzp92e8kj_i6q4ync0x-hvucvwnevb9i9nnrnyxbayh8ge.png\"\n",
    "width=\"300\" height=\"200\">\n",
    "\n",
    "ReLu (Rectified Linear Unit):<br>\n",
    "<img src=\"https://media.datacamp.com/cms/google/ad_4nxdm3mjfeqgnkibwih8jqb9p93eqd1zmoasqf17atrjzvc7vyafjt2d5lvglvfy9tbuy86dsd7uijmrak-nqpqmfniawbkirncuwlyspzojwo6ta6xrda1mcq-dkuizktyc6jk4peni3evumlow5lkpywah1.png\"\n",
    "width=\"300\" height=\"200\">\n",
    "<br>\n",
    "[More Details](https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpK8RU0qfNIB"
   },
   "source": [
    "# Project #4\n",
    "\n",
    "\n",
    "Please see BrightSpace."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
