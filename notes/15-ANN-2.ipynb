{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"eYWaaaNJfNH1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638453584524,"user_tz":300,"elapsed":874,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"f407539e-585f-461d-999f-d732f1a2dada"},"source":["###### Set Up #####\n","# verify our folder with the data and module assets is installed\n","# if it is installed make sure it is the latest\n","!test -e ds-assets && cd ds-assets && git pull && cd ..\n","# if it is not installed clone it \n","!test ! -e ds-assets && git clone https://github.com/lutzhamel/ds-assets.git\n","# point to the folder with the assets\n","home = \"ds-assets/assets/\" \n","import sys\n","sys.path.append(home)      # add home folder to module search path"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Already up to date.\n"]}]},{"cell_type":"markdown","metadata":{"id":"hLDTyHiyfNH9"},"source":["# Constructing a basic ANN/MLP\n","\n","Let's build some MLPs.  A fundamental problem with MLP design is the sheer number of design possibilities of these models.  The MLP classisfier as part of the sklearn package has 23 (!) tunable paramters.  The good news is that all of these parameters except for the architectural parameters and the maximum number of training iterations have good default values. For the architectural parameters a good starting point is an MLP  with a single hidden layer where the number of nodes in the hidden layer is computed as follows,\n","\n","$ \\#\\mbox{nodes} = 2 \\times \\#\\mbox{vars}$\n","\n","That is the number of hidden nodes is twice the number of independent variables in the training data.  For the maximum number of training iterations we simply choose a very large value, e.g. 10,000. Let's try this using the breast cancer dataset,"]},{"cell_type":"code","metadata":{"id":"74XIcu6lLroz"},"source":["# set up\n","import pandas as pd\n","import numpy as np\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from confint import classification_confint"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FUyuHeUiLvn8","executionInfo":{"status":"ok","timestamp":1638453584853,"user_tz":300,"elapsed":5,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"757424e6-587c-4e12-ca44-44f658ac52f0"},"source":["# get data\n","df = pd.read_csv(home+\"wdbc.csv\")\n","df = df.drop(['ID'],axis=1)\n","\n","X  = df.drop(['Diagnosis'],axis=1)\n","y = df['Diagnosis']\n","\n","print(\"Shape: {}\".format(X.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape: (569, 30)\n"]}]},{"cell_type":"markdown","metadata":{"id":"6bakptkLkR3Y"},"source":["Looking at the shape of the training data we see that there are 30 independent variables. Applying our rule from above means that we should construct an MLP with a single hidden layer that contains 60 nodes."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EsFdJqQr8Za1","executionInfo":{"status":"ok","timestamp":1638453586287,"user_tz":300,"elapsed":1437,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"32ca8725-2cf3-4760-bb78-923ae54fb348"},"source":["# neural network\n","model = MLPClassifier(hidden_layer_sizes=(60,), max_iter=10000, random_state=1)\n","\n","# train and test the model\n","model.fit(X, y)\n","predict_y = model.predict(X)\n","acc = accuracy_score(y, predict_y)\n","lb, ub = classification_confint(acc, X.shape[0])\n","print(\"Accuracy: {:3.2f} ({:3.2f}, {:3.2f})\".format(acc, lb, ub))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.95 (0.93, 0.96)\n"]}]},{"cell_type":"markdown","metadata":{"id":"191fBfjf32tc"},"source":["The accuracy of this classifier is encouraging given that we constructed it using just our rule of thumb.  Also, you might be surprised in that we are using the whole data set both as training as well as testing data.  In this instance that is ok because we are not performing a model search, we simply want to see how our rule of thumb performs.  If we were performing a model search then we would have to resort to train-test splits or cross-validation as we do in the grid search below."]},{"cell_type":"markdown","metadata":{"id":"rUORcmWDfNIA"},"source":["# MLP Grid Search\n","\n","We can also perform a grid search to find the optimal network. However, beware that a grid search over all possible parameters of an MLP is almost impossible:  Too many different combinations possible and training MLPs is sloooowwww.  To mitigate this we concentrate on a couple of key parameters to search over (see the comments in the code)."]},{"cell_type":"code","metadata":{"id":"655pgMsffNIA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638453718611,"user_tz":300,"elapsed":132329,"user":{"displayName":"Lutz Hamel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGw3Lpyn8v_49EbZpajVWtDhieE3O3Dn1YoG2yfQ=s64","userId":"10287662568849688016"}},"outputId":"0f862e30-d466-455f-f625-2506259639d6"},"source":["# set up\n","import pandas as pd\n","import numpy as np\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","from sklearn.model_selection import GridSearchCV\n","from confint import classification_confint\n","\n","# get data\n","df = pd.read_csv(home+\"wdbc.csv\")\n","df = df.drop(['ID'],axis=1)\n","X  = df.drop(['Diagnosis'],axis=1)\n","y = df['Diagnosis']\n","\n","# neural network\n","model = MLPClassifier(max_iter=10000, random_state=1)\n","\n","# grid search\n","# We set up a grid search over the architecture and activation functions.\n","# In the architecture search we limit ourselves to node values that are multiples\n","# of the number of independent variables in the training data.  Also, we\n","# limit ourselves to a maximum of two hidden layers.\n","param_grid = {\n","    # search over different architectures\n","    'hidden_layer_sizes': \n","      [ \n","      (30,), (60,), (120,),            # single layer MLP\n","      (30,30), (30, 60), (30, 120),    # 2 layers, first 30, second varying\n","      (60, 30), (60,60), (60, 120),    # 2 layers, first 60, second varying\n","      (120, 30), (120, 60), (120, 120) # 2 layers, first 120, second varying\n","      ],\n","    # search different activation functions\n","    'activation' : ['logistic', 'tanh', 'relu']   \n","}\n","\n","# use 3-fold cross-validation otherwse grid search takes too long\n","grid = GridSearchCV(model, param_grid, cv=3) \n","grid.fit(X, y)\n","print(\"Grid Search: best parameters: {}\".format(grid.best_params_))\n","\n","# evaluate the best model\n","best_model = grid.best_estimator_\n","predict_y = best_model.predict(X)\n","acc = accuracy_score(y, predict_y)\n","lb,ub = classification_confint(acc,X.shape[0])\n","print(\"Accuracy: {:3.2f} ({:3.2f},{:3.2f})\".format(acc,lb,ub))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Grid Search: best parameters: {'activation': 'logistic', 'hidden_layer_sizes': (30, 30)}\n","Accuracy: 0.96 (0.94,0.97)\n"]}]},{"cell_type":"markdown","metadata":{"id":"j8RWVg-GTmGf"},"source":["Interestingly enough, this network constructed using the graidsearch is a network with two hidden layers each with 30 nodes in it."]},{"cell_type":"markdown","metadata":{"id":"CsUu-8bMfNIB"},"source":["# Model Comparison\n","\n","The accuracy of the network we constructed using our rule of thumb was,\n","```\n","95% (93%, 96%)\n","```\n","and the accuracy of our network constructed using a grid search was,\n","```\n","96% (94%, 97%)\n","```\n","Our first observation is that our rule of thumb got us pretty close to the performance of our optimized network.\n","The second observation is that the difference in accuracy between these two models is statistically not significant because their confidence intervals overlap.  In this case we might look at other criteria such as the confusion matrix to choose from these two models."]},{"cell_type":"markdown","metadata":{"id":"mpK8RU0qfNIB"},"source":["# Team Exercise\n","\n","\n","An exercise in artificial neural networks.  For more details please see BrightSpace Assignment #4"]}]}