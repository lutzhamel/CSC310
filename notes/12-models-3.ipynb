{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3405,
     "status": "ok",
     "timestamp": 1708971122830,
     "user": {
      "displayName": "Lutz Hamel",
      "userId": "10287662568849688016"
     },
     "user_tz": 300
    },
    "id": "LlVbRD8lY5nI",
    "outputId": "40a3974e-f8f3-49df-ea70-b316144e4777"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "###### Config #####\n",
    "import sys, os, platform\n",
    "if os.path.isdir(\"ds-assets\"):\n",
    "  !cd ds-assets && git pull\n",
    "else:\n",
    "  !git clone https://github.com/lutzhamel/ds-assets.git\n",
    "colab = True if 'google.colab' in os.sys.modules else False\n",
    "system = platform.system() # \"Windows\", \"Linux\", \"Darwin\"\n",
    "home = \"ds-assets/assets/\"\n",
    "sys.path.append(home)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook level imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dsutils\n",
    "np.set_printoptions(formatter={'float_kind':\"{:3.2f}\".format})\n",
    "from sklearn import tree\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "az04_f5PY5nQ"
   },
   "source": [
    "# Model Building and Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Br5Q9ND9Y5nR"
   },
   "source": [
    "Building models carries with it a **certain amount of uncertainty**.\n",
    "Recall that machine learning is an inductive activity: We learn from examples and try to generalize by creating patterns/hypotheses/theories. We use datasets that represent **samples** from much\n",
    "larger domains in order to learn.  Recall the \"black swan problem\" where the overall domain of swans contains both white \n",
    "and black swans.  But the white swans outnumber the black swans by a substantial margin and therefore, if we are not careful, most samples \"D\" drawn from\n",
    "the overall population \"X\" will only contain white swans as can be seen in the figure below,\n",
    "\n",
    "<center>\n",
    "<img \n",
    "  src=\"https://raw.githubusercontent.com/lutzhamel/ds-assets/main/assets/black-swans.png\"  \n",
    "  height=\"200\" \n",
    "  width=\"240\">\n",
    "</center>\n",
    "\n",
    "This means, if we learn from those samples we will come to the incorrect conclusion that \"all swans are white\".\n",
    "\n",
    "What this example illustrates is that the quality of our model is very much dependent on the quality \n",
    "of the data samples.  Unfortunately, in most cases the machine learning practitioner has no control over\n",
    "the construction of the data samples. \n",
    "This quality of the sample representation of the domain is a constant source of uncertainty when building models.  We can actually observe this uncertainty even in our simple iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(home+\"iris.csv\")\n",
    "X  = df.drop(['id','Species'],axis=1)\n",
    "y = df['Species']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using train-test splits to build models and reporting the testing accuracy.  We do this five times randomly splitting the iris data into train and test folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0: 0.98\n",
      "Accuracy 1: 0.93\n",
      "Accuracy 2: 0.93\n",
      "Accuracy 3: 0.96\n",
      "Accuracy 4: 0.89\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "   model = tree.DecisionTreeClassifier(max_depth=3)\n",
    "   (X_train, X_test, y_train, y_test) = \\\n",
    "      model_selection.train_test_split(X, \n",
    "                                       y, \n",
    "                                       train_size=0.7, \n",
    "                                       test_size=0.3)\n",
    "   model.fit(X_train, y_train)\n",
    "   y_test_model = model.predict(X_test)\n",
    "   print(\"Accuracy {}: {:3.2f}\"\n",
    "         .format(i,metrics.accuracy_score(y_test, y_test_model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notice the impact the random splits have on the testing accuracy.  \n",
    "* Some splits give rise to good models and some splits not so much.  \n",
    "    \n",
    "> **Each split can be seen as randomly sampling a train and a test set from the original domain of all iris flowers**. \n",
    "\n",
    "* Here we are directly observing the effects of the uncertainty due to the data samples.\n",
    "\n",
    "* This uncertainty reflects into our models. \n",
    "   * If our data is a poor representation of the domain then the models we construct using it will generalize poorly. \n",
    "   * If our  data is a good representation of the domain then we can expect that our model will generalize well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use **confidence intervals** in order to quantify the uncertainty discussed above in our model evaluations.\n",
    "\n",
    "Let us define confidence intervals formally. \n",
    "Given a model accuracy, **acc**, then the confidence interval is defined as the probability **p** that our model accuracy **acc** lies between some lower bound **lb** and some upper bound **ub**,\n",
    "\n",
    "$$\n",
    "Pr(lb \\le acc \\le ub) = p.\n",
    "$$\n",
    "\n",
    "Paraphrasing this equation with *p = 95%*:\n",
    "\n",
    "> We are 95% percent sure that our model accuracy **acc** is not worse than **lb** and not better than **ub**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AU9ejtiY5nU"
   },
   "source": [
    "Ultimitely we are interested in the lower and upper bounds of the 95% confidence interval.  We can use the following formulas to compute the bounds:\n",
    "\n",
    "$$ub = acc + 1.96 \\sqrt \\frac{acc (1 - acc)}{n}$$\n",
    "\n",
    "$$lb = acc - 1.96 \\sqrt \\frac{acc (1 - acc)}{n}$$\n",
    "\n",
    "Here, *n* is the number of observations in the testing dataset used to estimate *acc*. The constant 1.96 is called the *z-score* and expresses the fact that we are computing the 95% confidence interval.\n",
    "\n",
    "Notice that as we let $n \\rightarrow \\infty$ both the upper bound and the lower bound tend towards the accuracy.  That is, as we test the model on more and more testing points we become more and more confident that the given accuracy this the correct accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZrS6gvHY5nV"
   },
   "source": [
    "Let's do an actual example using our iris dataset.  We want to print out the  accuracy together with it's 95% confidence interval. \n",
    "\n",
    "We construct a best model and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9823,
     "status": "ok",
     "timestamp": 1708971135667,
     "user": {
      "displayName": "Lutz Hamel",
      "userId": "10287662568849688016"
     },
     "user_tz": 300
    },
    "id": "HjHjnvrV6CH9",
    "outputId": "57d4f48b-5961-44c8-bc59-3999b3771e1f"
   },
   "outputs": [],
   "source": [
    "depth_ceiling = tree.DecisionTreeClassifier(max_depth=None)\\\n",
    "   .fit(X,y).get_depth()\n",
    "model = tree.DecisionTreeClassifier(random_state=2)\n",
    "param_grid = {\n",
    "    'max_depth': list(range(1,depth_ceiling+1)),               \n",
    "    }\n",
    "best_model = model_selection.GridSearchCV(model,param_grid).fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy: 0.97 (0.95, 1.00)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsutils.acc_score(best_model, X, y, as_string=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxsPRGorY5nW"
   },
   "source": [
    "# Regression Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tL0IOclY5nW"
   },
   "source": [
    "When performing regression we use the $R^2$ score to examine the quality of our models.  Given that we only use a small training dataset for fitting the model compared to the rest of the data universe it is only natural to ask what the 95% confidence interval for this score might be.  We have a formula for that -- it is not as straight forward as the confidence interval for classification,\n",
    "\n",
    "$$lb = R^2 - 2\\sqrt{\\frac{4R^{2}(1-R^{2})^{2}(n-k-1)^{2}}{(n^2 - 1)(n+3)}}$$\n",
    "\n",
    "$$ub = R^2 + 2\\sqrt{\\frac{4R^{2}(1-R^{2})^{2}(n-k-1)^{2}}{(n^2 - 1)(n+3)}}$$\n",
    "\n",
    "Here, *n* is the number of observations in the validation/testing dataset and *k* is the number of independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzIOt6KJY5nX"
   },
   "source": [
    "Let's look at an actual regression problem and compute the $R^2$ score and it's 95% confidence interval. We will use the cars problem from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 982,
     "status": "ok",
     "timestamp": 1708971136636,
     "user": {
      "displayName": "Lutz Hamel",
      "userId": "10287662568849688016"
     },
     "user_tz": 300
    },
    "id": "bfrHjJj5nUKb",
    "outputId": "7ff9d3fb-94b8-4a24-e149-118f7d52346e"
   },
   "outputs": [],
   "source": [
    "# get our dataset\n",
    "cars_df = pd.read_csv(home+\"cars.csv\")\n",
    "X = cars_df[['speed']]\n",
    "y = cars_df['dist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a regression model \n",
    "model = tree.DecisionTreeRegressor().fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R^2 Score: 0.79 (0.69, 0.89)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsutils.rs_score(model, X, y, as_string=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcIGuoRqY5nY"
   },
   "source": [
    "# Statistical Significance\n",
    "\n",
    "Besides giving us an idea of the uncertainty of our model the 95% confidence intervals also have something to say about the significance of scores of different models:  \n",
    "\n",
    "> If the confidence intervals overlap then the difference in model performance of two different models on the same dataset is **not statistically significant**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7zuIKNZklPm"
   },
   "source": [
    "## A Worked Example\n",
    "\n",
    "Here we use a real-world dataset that tries to predict the sex of abalone given a set of parameters.\n",
    "First we will construct the optimal model and then we construct a tree with minimal complexity for the same data set and compare the performances using statistical significance.\n",
    "\n",
    "The optimal tree first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5246,
     "status": "ok",
     "timestamp": 1708971141879,
     "user": {
      "displayName": "Lutz Hamel",
      "userId": "10287662568849688016"
     },
     "user_tz": 300
    },
    "id": "mrnvMlNFkjDM",
    "outputId": "20509d94-02e1-4ab1-bde5-8156bd8d4891"
   },
   "outputs": [],
   "source": [
    "# get the abalone data\n",
    "df = pd.read_csv(home+\"abalone.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic descriptive statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4177, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole_weight</th>\n",
       "      <th>shucked_weight</th>\n",
       "      <th>viscera_weight</th>\n",
       "      <th>shell_weight</th>\n",
       "      <th>rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sex  length  diameter  height  whole_weight  shucked_weight  viscera_weight  \\\n",
       "0   M   0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
       "1   M   0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
       "2   F   0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
       "3   M   0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
       "4   I   0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
       "\n",
       "   shell_weight  rings  \n",
       "0         0.150     15  \n",
       "1         0.070      7  \n",
       "2         0.210      9  \n",
       "3         0.155     10  \n",
       "4         0.055      7  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex\n",
       "M      1528\n",
       "I      1342\n",
       "F      1307\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['sex']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct our data matrices\n",
    "X  = df.drop(columns=['sex'])\n",
    "y = df[['sex']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct our optimal tree first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal tree\n",
    "depth_ceiling = tree.DecisionTreeClassifier(max_depth=None)\\\n",
    "    .fit(X,y)\\\n",
    "    .get_depth()\n",
    "model = tree.DecisionTreeClassifier()\n",
    "param_grid = {'max_depth': list(range(1,depth_ceiling+1))}\n",
    "best_model = model_selection\\\n",
    "    .GridSearchCV(model, param_grid)\\\n",
    "    .fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy: 0.59 (0.57, 0.60)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsutils.acc_score(best_model, X, y, as_string=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZtiugV8lVOc"
   },
   "source": [
    "Now we construct the minimal tree with max depth of 2.  We chose two because at minimum we need two nested if-then-else statements in order to distinguish three different labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1708971142076,
     "user": {
      "displayName": "Lutz Hamel",
      "userId": "10287662568849688016"
     },
     "user_tz": 300
    },
    "id": "-zXdGYhjlcsx",
    "outputId": "d5a6fef5-60af-4aa2-9f83-04be8575de8b"
   },
   "outputs": [],
   "source": [
    "# minimal complexity tree: depth 2\n",
    "# create our model object\n",
    "model = tree\\\n",
    "   .DecisionTreeClassifier(max_depth=2)\\\n",
    "   .fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy: 0.54 (0.52, 0.55)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsutils.acc_score(model, X, y, as_string=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWjE01FhmF6V"
   },
   "source": [
    "**Observation**: The confidence intervals are not overlapping, therefore **the performance difference is statistically significant**! That means the optimal model indeed performs better than the minimal tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test vs. Refit Scores\n",
    "\n",
    "Let show that there is no statistically significant difference between the testing score computed with train-test partitions and the refit score.  We'll use the iris data set to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(home+\"iris.csv\")\n",
    "X  = df.drop(['id','Species'],axis=1)\n",
    "y = df['Species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max depth of tree\n",
    "depth_ceiling = tree.DecisionTreeClassifier(max_depth=None)\\\n",
    "    .fit(X,y)\\\n",
    "    .get_depth()\n",
    "\n",
    "# prototype model\n",
    "model = tree.DecisionTreeClassifier()\n",
    "\n",
    "# parameter grid for our searches\n",
    "param_grid = {'max_depth': list(range(1,depth_ceiling+1))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, find the best model using train-test partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Accuracy: 0.96 (0.90, 1.00)\n"
     ]
    }
   ],
   "source": [
    "(X_train, X_test, y_train, y_test) = model_selection\\\n",
    "   .train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=1)\n",
    "\n",
    "best_model = model_selection\\\n",
    "    .GridSearchCV(model, param_grid)\\\n",
    "    .fit(X_train,y_train)\n",
    "\n",
    "acc,lb,ub = dsutils.acc_score(best_model,X_test,y_test)\n",
    "print(f\"Train-Test Accuracy: {acc:.2f} ({lb:.2f}, {ub:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, find the best model using the whole dataset and evaluate using the refit score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Data Accuracy: 0.99 (0.98, 1.00)\n"
     ]
    }
   ],
   "source": [
    "best_model = model_selection\\\n",
    "    .GridSearchCV(model, param_grid)\\\n",
    "    .fit(X,y)\n",
    "acc,lb,ub = dsutils.acc_score(best_model,X,y)\n",
    "print(f\"Full Data Accuracy: {acc:.2f} ({lb:.2f}, {ub:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: The score difference between those two evaluation methods ist **not** statically significant.  Therefore we can use either one to find and evaluate our best models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNDtX6Zumcm5"
   },
   "source": [
    "# Project\n",
    "\n",
    "Please see BrightSpace for project #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm\n",
    "\n",
    "The midterm will cover everything up to and including the material in project #3"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
