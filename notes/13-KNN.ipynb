{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 343,
     "status": "ok",
     "timestamp": 1709256829891,
     "user": {
      "displayName": "Lutz Hamel",
      "userId": "10287662568849688016"
     },
     "user_tz": 300
    },
    "id": "GOTn9R3We-Y1",
    "outputId": "e48dd24a-8da8-4388-888b-6e7e5bfee9ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "###### Config #####\n",
    "import sys, os, platform\n",
    "if os.path.isdir(\"ds-assets\"):\n",
    "  !cd ds-assets && git pull\n",
    "else:\n",
    "  !git clone https://github.com/lutzhamel/ds-assets.git\n",
    "colab = True if 'google.colab' in os.sys.modules else False\n",
    "system = platform.system() # \"Windows\", \"Linux\", \"Darwin\"\n",
    "home = \"ds-assets/assets/\"\n",
    "sys.path.append(home)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook level imports\n",
    "import pandas as pd\n",
    "import dsutils                        # classification_confint\n",
    "from sklearn import neighbors         # KNeighborsClassifier\n",
    "from sklearn import tree              # DecisionTreeClassifier\n",
    "from sklearn import metrics           # accuracy_score, confusion_matrix\n",
    "from sklearn import model_selection   # train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYURltXbe-Y-"
   },
   "source": [
    "# k-NN Classification\n",
    "\n",
    "k-NN: **k** **N**earest **N**eighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MY9iVDxne-Y_"
   },
   "source": [
    "In k-NN classification the label of an **unknown instance** is computed from a simple majority vote of the **nearest neighbors of that point***: a query point is assigned the label which has the most representatives within the nearest neighbors of that point.\n",
    "\n",
    "K-NN classification is a type of **instance-based learning**: In **instance-based learning** we do not attempt to construct an internal model, but simply view the **instances of the training data as the model**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibqp6g_5e-ZA"
   },
   "source": [
    "## An Illustration\n",
    "\n",
    "Consider the following figure,\n",
    "\n",
    "<!-- ![knn](assets/knn.png) -->\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/lutzhamel/ds-assets/main/assets/knn.png\" height=\"256\" width=\"280\">\n",
    "</center>\n",
    "\n",
    "We want to assign the unknown point either to the class of blue squares or to the class of red triangles,\n",
    "\n",
    "* If k = 3 (solid line circle) it is assigned to the class of red triangles because there are 2 triangles and only 1 square inside the inner circle.\n",
    "\n",
    "* If k = 5 (dashed line circle) it is assigned to the class of blue squares (3 squares vs. 2 triangles inside the dashed circle).\n",
    "\n",
    "**Note**: The value k is a model parameter and model accuracy depends on this parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOFR_NDue-ZA"
   },
   "source": [
    "## A Worked Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lW41YKJie-ZA"
   },
   "source": [
    "Let's build an k-NN classifier for the iris dataset.  \n",
    "\n",
    "**NOTE**: we are not searching for the optimal model, we just want to build a classifier and pick a value for k that seems appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2405,
     "status": "ok",
     "timestamp": 1709256832294,
     "user": {
      "displayName": "Lutz Hamel",
      "userId": "10287662568849688016"
     },
     "user_tz": 300
    },
    "id": "npqBRyHfe-ZB",
    "outputId": "4888ceee-6b4f-4703-cc61-1788fbf40f35"
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "df = pd.read_csv(home+\"iris.csv\")\n",
    "X  = df.drop(columns=['id','Species'])\n",
    "y = df['Species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a model with default settings\n",
    "model = neighbors.KNeighborsClassifier().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97 (0.94, 1.00)\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "acc,lb,ub = dsutils.acc_score(model,X,y)\n",
    "print(\"Accuracy: {:3.2f} ({:3.2f}, {:3.2f})\".format(acc, lb, ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY_EDc4J45hZ"
   },
   "source": [
    "The performance is not bad for a default model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5zeXrzde-ZC"
   },
   "source": [
    "# Model Comparison\n",
    "\n",
    "Here we are a little bit more careful with our model construction and do a cross-validated grid search for the optimal value of k.\n",
    "Furthermore we want to see how our optimal k-NN classifier performance stacks up to the performance of an optimal decision tree model in a statistical valid manner.\n",
    "\n",
    "\n",
    "Letâ€™s work our way through this comparison using the `wdbc` dataset:\n",
    "\n",
    "* Build optimal k-NN and tree models using grid search\n",
    "* Compute the accuracy for the classifiers\n",
    "* Print out the confusion matrix for each classifier\n",
    "* Print out the confidence interval for each classifier\n",
    "* Decide if the difference between classifiers is statistically significant or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrhiXUR9e-ZD"
   },
   "source": [
    "## Set Up\n",
    "\n",
    "Get our training data and format in way that `sklearn` expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1709256832294,
     "user": {
      "displayName": "Lutz Hamel",
      "userId": "10287662568849688016"
     },
     "user_tz": 300
    },
    "id": "II8S6swxW2su"
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "df = pd.read_csv(home+\"wdbc.csv\").drop(columns=['ID'])\n",
    "\n",
    "# format training data for sklean\n",
    "X  = df.drop(columns=['Diagnosis'])\n",
    "y = df['Diagnosis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NnqV3j2e-ZE"
   },
   "source": [
    "## k-NN Classifier\n",
    "\n",
    "First up is the k-NN classifier.  In order to find the optimal model we set up a grid search over the number of neighbors.  In this case we search the values from 1 to 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8029,
     "status": "ok",
     "timestamp": 1709256840321,
     "user": {
      "displayName": "Lutz Hamel",
      "userId": "10287662568849688016"
     },
     "user_tz": 300
    },
    "id": "wyT0W1J1e-ZE",
    "outputId": "bc896d1b-4077-49e4-84db-ebb2f91b0705"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94 (0.92, 0.96)\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "model = neighbors.KNeighborsClassifier()\n",
    "param_grid = {'n_neighbors': list(range(1,26))}   # k = 1..25\n",
    "best_model = model_selection\\\n",
    "   .GridSearchCV(model, param_grid)\\\n",
    "   .fit(X, y)\\\n",
    "   .best_estimator_\n",
    "acc,lb,ub = dsutils.acc_score(best_model,X,y)\n",
    "print(\"Accuracy: {:3.2f} ({:3.2f}, {:3.2f})\".format(acc, lb, ub))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "     M    B\n",
      "M  186   26\n",
      "B    8  349\n"
     ]
    }
   ],
   "source": [
    "# build the confusion matrix for more detailed error analysis\n",
    "predict_y = best_model.predict(X)\n",
    "labels = ['M', 'B']\n",
    "cm = metrics.confusion_matrix(y, predict_y, labels=labels)\n",
    "cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "print(\"Confusion Matrix:\\n{}\".format(cm_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhlWnvWPJkbf"
   },
   "source": [
    "Let's take a look at the performance data.  In terms of accuracy we see that the best k-NN model has an accuracy of 94% with a confidence interval of (92%, 96%).  From a medical application perspective the confusion matrix is worrisome.  We see that of the 212 malignant samples the model misclassifies 26 as benign.  This kind of error is called the 'false negative' error and in this case would mean that 12% of the malignant cases remain undetected. We also see that of the 357 benign samples it misclassifies 8 as malignant.  The is called the 'false positive' error. From a medical point of view this is not as worrisome because additional tests will identify these cases correctly as benign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6g1xxMaPe-ZD"
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "For decision trees we set up a grid search over the tree depth from 1 to 20 and the criterion which searches over `entropy` and `gini`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8045,
     "status": "ok",
     "timestamp": 1709256848344,
     "user": {
      "displayName": "Lutz Hamel",
      "userId": "10287662568849688016"
     },
     "user_tz": 300
    },
    "id": "USmn4hFMe-ZD",
    "outputId": "03536950-c2dc-4864-90e5-6e2a96b3503f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98 (0.97, 0.99)\n"
     ]
    }
   ],
   "source": [
    "# decision trees\n",
    "model = tree.DecisionTreeClassifier(random_state=1)\n",
    "param_grid = {'max_depth': list(range(1,21)), 'criterion': ['entropy','gini'] }\n",
    "best_model = model_selection\\\n",
    "   .GridSearchCV(model, param_grid)\\\n",
    "   .fit(X, y)\\\n",
    "   .best_estimator_\n",
    "acc,lb,ub = dsutils.acc_score(best_model,X,y)\n",
    "print(\"Accuracy: {:3.2f} ({:3.2f}, {:3.2f})\".format(acc, lb, ub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "     M    B\n",
      "M  210    2\n",
      "B    7  350\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# build the confusion matrix for a more detailed error analysis\n",
    "predict_y = best_model.predict(X)\n",
    "labels = ['M', 'B']\n",
    "cm = metrics.confusion_matrix(y, predict_y, labels=labels)\n",
    "cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "print(\"Confusion Matrix:\\n{}\".format(cm_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82db8d64YJgi"
   },
   "source": [
    "The performance of the decision tree model is much better overall and from a medical point specifically.  Less than 1% of the malignant cases is classified as a 'false negative' giving much more confidence in its applicability in a medical setting. The accuracy of the model is 98% with a confidence interval of (97%, 99%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BNwcln4e-ZF"
   },
   "source": [
    "## Performance Comparison and Model Selection\n",
    "\n",
    "If we compare models we have to look beyond the raw performance numbers in this case 94% and 98% for k-NN and the decision tree model, respectively. We have to ask if the difference in performance between these two models is statistically significant.  Consider the performance of the k-NN model with an accuracy and confidence interval of,\n",
    "```\n",
    "94% (92%, 96%)\n",
    "```\n",
    "Also consider the performance of the decision tree model with an accuracy and confidence interval of,\n",
    "```\n",
    "98% (97%, 99%)\n",
    "```\n",
    "Here we see that\n",
    "the confidence intervals for the decision tree and the K-NN classifier **do not overlap**.  That means here the decision tree is truly the better model and the performance difference between the two models is **statistically significant**.  \n",
    "\n",
    "**Observation**: Therefore we will select the **decision tree as a model** for our breast cancer data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1709256848345,
     "user": {
      "displayName": "Lutz Hamel",
      "userId": "10287662568849688016"
     },
     "user_tz": 300
    },
    "id": "8bGNDOW8e-ZF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
